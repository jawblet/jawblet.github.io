---
layout: post
title: Week 2
---

This week I worked on generating the naive algorithm and benchmarking it. Using a sample table called `orders`, I generated intervention matrices for two attributes in that table--order priority and order status.

After the discussion with Prof Wu and Haneen at the beginning of the week, I understood the goal was to limit transfer from DuckDB as much as possible. To create the naive algorithm I did the following:

1. Wrote a query to fetch all unique values per attribute.
2. Used the list of unique attributes to construct a SELECT query where each column corresponds to one value per attributes.
3. Create a table given this query.

I did this for two attributes, and then to create conjunctions between two values of different attributes, I cross join the tables from each query. The resulting table is every combination of values between the two attributes. 

I then began to time how long these queries take to execute given different scale factors of the database. 

```
### QUERY 1 ###
        q1 = f"SELECT DISTINCT {attr1} FROM {table}"
        dist1 = con.execute(q1).df()
        DropLineageTables(con)
​
        start1 = timer()
        d1 = dist1[attr1].to_numpy()
   
        attr1_conditions = []
        for i, value in enumerate(d1):
            condition = f"{attr1} = '{value}'"
            attr1_conditions.append(condition)
​
        query1 = f"CREATE TABLE {attr1}_view AS SELECT rowid, {', '.join(attr1_conditions)} FROM {table};"
        con.execute(query1)
        end1 = timer()
        print(f"{len(attr1_conditions)} interventions: {(end1-start1)}")
```
