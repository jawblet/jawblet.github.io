---
layout: post
title: Week 1
---

To start working on the project, I began by reading (in some cases re-reading) papers that describe the project or related work. I read:

  - SMOKE: Fine-Grained Lineage At Interactive Speed
  - FaDE: Answering “Why?” Made Fast 
  - Scorpion: Explaining Away Outliers in Aggegrate Queries 

The Scorpion paper focuses on predicate generation that explains a subset of results that have been flagged as an outlier. In this context, a predicate "explains" a result if applying the predicate to the input data removes the user-defined outlier from the output, while minimally effecting the rest of the result. The paper describes the challenges inherent to finding the predicate that most influences the outlier, and possible solutions. The part most relevant to my work is the section on **partitioning**, which describes how the algorithm generates a ranked list of predicates.

My first task next week is to implement something like the naive partitioning algorithm, described here: 

> The NAIVE algorithm first defines all distinct single-attribute clauses, then enumerates all conjunctions of up to one clause from each attribute. The clauses over a discrete attribute, Ai , are of the form, “A_i in (· · · )” where the · · · is replaced with all possible combinations of the attribute’s distinct values. Clauses over continuous attributes are constructed by splitting the attribute’s domain into a fixed number of equi-sized ranges, and enumerating all combinations of consecutive ranges.

The FaDe paper focuses more on the speed of generating predicates, and how **lineage**, or metadata on the relationship between input and output tuples, can enable faster results. The paper also mentions selection matrices, which represent the outcomes of counterfactual deletions. There are many possible optimizations, such as minimizing how much space each matrix cell takes. Generating a selection matrix will also be a part of my task next week. In the coming weeks, I will attempt to optimize it. 

In addition to reading the papers, I'm refreshing my Python skills, and familiarizing myself with Pandas, NumPy, and DuckDB.





