---
layout: post
title: Week 7
---

This week I coded a naive + vectorized version of the encoding scheme discussed last week. 

Naive version: I used the CreateScalarFunction method from DuckDB, which operates on individual values (taking one as input and returning one as output). This version has 1500 UDF calls for 1500 attributes. 

```
int64_t udf(int64_t key, int val) {
	int r = 64; 
  	int i = key / r; 

  	unsigned long long b = 1ULL;
  	if(key == val) {
    	int n = r - (key - (r * i));
    	b <<= n;
  	} else {
    		b = 0ULL;
  	}

  return b;
```

I benchmarked the performance of this UDF with 1500 interventions for scale factors 0.001, 0.01, 0.1, and 1. 

Vectorized version: I used the CreateVectorizedFunction, which takes tuples as input data, performs the UDF operation on chunk in a vectorized manner, and returns a result vector. This version has 1500/64 UDF calls for 1500 attributes.

```
con.CreateVectorizedFunction("udf", {LogicalType::BIGINT, LogicalType::INTEGER}, LogicalType::BIT, &udf);
 
string q = "SELECT udf(key, 0), udf(key, 1), udf(key, 2), udf(key, 3), udf(key, 4), udf(key, 5), udf(key, 6), udf(key, 7), udf(key, 8), udf(key, 9), udf(key, 10), udf(key, 11), udf(key, 12), udf(key, 13), udf(key, 14), udf(key, 15), udf(key, 16), udf(key, 17), udf(key, 18), udf(key, 19), udf(key, 20), udf(key, 21), udf(key, 22), udf(key, 23) FROM k;";
```

The UDF:

```
void udf(DataChunk &args, ExpressionState &state, Vector &result) {
  int r = 64;   

  // get ptrs to input and result vector
  result.SetVectorType(VectorType::FLAT_VECTOR);
  auto result_data = (bitset<64> *)FlatVector::GetData(result);
  auto input_data = (int64_t *)FlatVector::GetData(args.data[0]);

  //get range of attributes 
  auto i = *(int *)FlatVector::GetData(args.data[1]);  

  // bitshift values in range
  for(int n = 0; n < args.size(); n++) {
    bitset<64> b{1};
    if(*input_data > r*i && *input_data <= (r*i + r)) {
      int shift = r - (*input_data - (r * i));
      b <<= shift; 
    } else {
      b.set(0, false);
    }
    result_data[n] = b; 
    input_data++;  
  }
}
```

Performance was about 25x better in same benchmark tests.

Some things I’m not sure about:

- Each average timing is the average of running the program 5 times, but I realized that the first run is consistently higher for SF=0.001 an 0.01. I’m not timing the time it takes to connect to the DB. Maybe the results for those SFs are cached??

- When calling udf(<tuples>, <i>) with 1500 tuples, the udf splits <tuples> into 4 different datachunks, where the chunk’s vectors have 1262, 219, 18, and 1 rows (1262 + 219 + 18 + 1 = 1500). I’m not sure why this is the default behavior but will try to figure out.
