---
layout: post
title: Week 3
---

I’ve been working on benchmarking and exploring how duckdb stores the intervention matrices. I’ve been running benchmark tests on creating intervention matrices for the o_totalprice attributes with different numbers of interventions at scale factors 0.001, 0.01, 0.1, and 1. For each test, I started the number of interventions at 1 and increased the number by 500 until I reached the maximum number of interventions for that scale or 50000. 

I found that for all scale factors, to create a view for any number of interventions seems to take the same amount of time. Interestingly, querying all rows in this view (SELECT * FROM view) also seemed to take the take the same amount of time per scale factor and the time to do so seemed to increase linearly (at least from 1 to 50000). 

When creating a table instead of a view, it seems that the time to create a table increases dramatically as the number of interventions increases at higher scale factors. However, by playing around with this, it seems like the time to create the table also increases dramatically when there is limited memory. At times, creating large tables would fill the space on my disk, but when I cleared more space, those same tables seemed to be created in less time. If I’m interpreting this correctly, really understanding this relationship will require testing in an environment with more available space. 
